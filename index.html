<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8" />
  <title>ArcFace 512D 1顔認識デモ</title>
  <style>
    body { font-family: sans-serif; text-align: center; margin: 0; background: #f7f7f7; }
    h2 { margin-top: 24px; }
    #controls { margin: 20px 0; }
    input, button { padding: 6px 10px; margin: 4px; }
    #video { border: 1px solid #333; }
    #overlay { position: absolute; top: 0; left: 0; }
    #container { position: relative; display: inline-block; }
    #status { margin-top: 12px; font-size: 1rem; }
  </style>
</head>
<body>
  <h2>ArcFace 512D 1顔認識デモ</h2>
  <div id="controls">
    <input type="file" id="photoUpload" accept="image/*" />
    <input type="text" id="nameInput" placeholder="名前を入力" />
    <button id="registerBtn">登録</button>
    <button id="startBtn" disabled>カメラ開始</button>
  </div>
  <div id="container">
    <video id="video" width="640" height="480" autoplay muted></video>
    <canvas id="overlay" width="640" height="480"></canvas>
  </div>
  <div id="status">モデル読込中…</div>
  <!-- 必要ライブラリ -->
  <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api/dist/face-api.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <script type="module">
    // 1. モデルのパス
    const ARC_ONNX_URL = "./w600k_mbf.onnx"; // アップロードしたファイル名に合わせて
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const ctx = overlay.getContext('2d');
    const statusEl = document.getElementById('status');
    const photoInput = document.getElementById('photoUpload');
    const nameInput = document.getElementById('nameInput');
    const regBtn = document.getElementById('registerBtn');
    const startBtn = document.getElementById('startBtn');
    let labeledName = null;
    let referenceVec = null;
    let session = null;

    // 2. モデル読み込み
    async function loadModels() {
      statusEl.textContent = 'モデル読込中…(顔検出)';
      await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model');
      statusEl.textContent = 'ArcFace モデル読込中…';
      session = await ort.InferenceSession.create(ARC_ONNX_URL);
      statusEl.textContent = '写真と名前を登録してください';
    }

    // 3. ArcFace埋め込み計算（ここをバグ修正版に）
    async function getArcFaceVec(image) {
      const det = await faceapi.detectSingleFace(image, new faceapi.TinyFaceDetectorOptions());
      if (!det) return null;
      const box = det.box;
      const faceCanvas = document.createElement('canvas');
      faceCanvas.width = 112; faceCanvas.height = 112;
      const fctx = faceCanvas.getContext('2d');
      fctx.drawImage(image, box.x, box.y, box.width, box.height, 0, 0, 112, 112);
      const imgData = fctx.getImageData(0, 0, 112, 112).data;
      let input = new Float32Array(1 * 3 * 112 * 112);
      for (let y = 0; y < 112; y++) {
        for (let x = 0; x < 112; x++) {
          for (let c = 0; c < 3; c++) {
            input[y * 112 * 3 + x * 3 + c] = imgData[(y * 112 + x) * 4 + c] / 255.0;
          }
        }
      }
      const tensor = new ort.Tensor("float32", input, [1, 3, 112, 112]);
      const feeds = {};
feeds['input.1'] = tensor; // モデルの実際の入力名に合わせる
const outputs = await session.run(feeds);

      // ★出力keyをconsoleで必ず確認
      console.log("ONNX outputs keys:", Object.keys(outputs));
      let emb =
        outputs['fc1'] ||
        outputs['embedding'] ||
        outputs['resnet_output'] ||
        outputs['output'] ||
        Object.values(outputs)[0];
      if (!emb || !emb.data) {
        console.error("ONNX embedding extraction failed. Outputs:", outputs);
        return null;
      }
      const norm = Math.sqrt(emb.data.reduce((s,v) => s+v*v, 0));
      const vec = emb.data.map(v => v/norm);
      return vec;
    }

    // 4. 登録ボタン
    regBtn.addEventListener('click', async () => {
      const file = photoInput.files[0];
      const label = nameInput.value.trim();
      if (!file || !label) {
        alert('写真と名前の両方を入力してください');
        return;
      }
      const img = await faceapi.bufferToImage(file);
      const vec = await getArcFaceVec(img);
      if (!vec) {
        statusEl.textContent = '顔が検出できません。別の写真でお試しください';
        return;
      }
      labeledName = label;
      referenceVec = vec;
      statusEl.textContent = `「${label}」を登録しました。カメラを開始できます`;
      startBtn.disabled = false;
    });

    // 5. カメラ開始
    startBtn.addEventListener('click', async () => {
      if (!referenceVec) {
        alert('先に写真と名前を登録してください');
        return;
      }
      startBtn.disabled = true;
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' } });
      video.srcObject = stream;
      statusEl.textContent = 'カメラ起動中…';
      video.onloadedmetadata = () => {
        video.play();
        runRecognition();
      };
    });

    // 6. コサイン類似度
    function cosineSim(a, b) {
      let s = 0;
      for(let i=0;i<a.length;i++) s += a[i]*b[i];
      return s;
    }

    // 7. 認識ループ
    async function runRecognition() {
      statusEl.textContent = '認識待機中…';
      const displaySize = { width: video.width, height: video.height };
      faceapi.matchDimensions(overlay, displaySize);

      setInterval(async () => {
        const det = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions());
        ctx.clearRect(0, 0, overlay.width, overlay.height);
        if (det) {
          const box = det.box;
          const faceCanvas = document.createElement('canvas');
          faceCanvas.width = 112; faceCanvas.height = 112;
          faceCanvas.getContext('2d').drawImage(video, box.x, box.y, box.width, box.height, 0, 0, 112, 112);
          const img = new window.Image();
          img.src = faceCanvas.toDataURL();
          img.onload = async () => {
            const vec = await getArcFaceVec(img);
            if (!vec) return;
            const sim = cosineSim(referenceVec, vec);
            const threshold = 0.4;
            let label = (sim > (1-threshold)) ? labeledName : 'Unknown';
            ctx.strokeStyle = (label === 'Unknown') ? 'red' : 'limegreen';
            ctx.lineWidth = 3;
            ctx.strokeRect(box.x, box.y, box.width, box.height);
            ctx.font = '20px sans-serif';
            ctx.fillStyle = ctx.strokeStyle;
            ctx.fillText(label, box.x, box.y > 24 ? box.y - 6 : box.y + 24);
          }
        }
      }, 400);
    }

    // 起動
    loadModels();
  </script>
</body>
</html>
